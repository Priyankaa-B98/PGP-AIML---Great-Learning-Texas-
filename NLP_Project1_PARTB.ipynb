{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaafd47",
   "metadata": {},
   "source": [
    "# PART_B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca6f74",
   "metadata": {},
   "source": [
    "#### DOMAIN: \n",
    "\n",
    "Customer support\n",
    "\n",
    "#### CONTEXT: \n",
    "\n",
    "Great Learning has a an academic support department which receives numerous support requests every day throughout the year. \n",
    "Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request to an actual human support executive if the request is complex or not in it’s database.\n",
    "\n",
    "\n",
    "#### DATA DESCRIPTION:\n",
    "\n",
    "A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills.\n",
    "\n",
    "#### PROJECT OBJECTIVE: Design a python based interactive semi - rule based chatbot which can do the following:\n",
    "\n",
    "- 1. Start chat session with greetings and ask what the user is looking for. [5 Marks]\n",
    "- 2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus. \n",
    "- 3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user    asks to end it.\n",
    "\n",
    "Hint: There are a lot of techniques using which one can clean and prepare the data which can be used to train a ML/DL classifier. Hence, it might require you to experiment, research, self learn and implement the above classifier. There might be many iterations between hand building the corpus and designing the best fit text classifier. As the quality and quantity of corpus increases the model’s performance i.e. ability to answer right questions also increases.\n",
    "\n",
    " Reference: https://www.mygreatlearning.com/blog/basics-of-building-an-artificial-intelligence-chatbot/\n",
    " \n",
    "#### Evaluation:\n",
    "\n",
    "Evaluator will use linguistics to twist and turn sentences to ask questions on the topics described in DATA DESCRIPTION and check if the bot is giving relevant replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6247a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c96a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1286eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a0c87",
   "metadata": {},
   "source": [
    "## Opening the .json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23265d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "\n",
    "f = open('C:\\\\Users\\\\ASUS\\\\GL Bot.json',)\n",
    "df = json.loads(f.read())\n",
    "bot_df = df['intents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156f46d",
   "metadata": {},
   "source": [
    "## Open a file for writing from GL_Bot.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file for writing\n",
    "\n",
    "GL_Bot = open('GLBot.txt', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c2d1a",
   "metadata": {},
   "source": [
    "## Creating the csv writer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106d6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the csv writer object\n",
    "csvwriter = csv.writer(GL_Bot)\n",
    "count = 0\n",
    "for value in bot_df:\n",
    "     if count == 0:\n",
    "            header = value.keys()\n",
    "            csvwriter.writerow(header)\n",
    "            count += 1\n",
    "csvwriter.writerow(value.values())\n",
    "GL_Bot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7a2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = open('GLBot.txt','r',errors = 'ignore')\n",
    "raw = dff.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bed421",
   "metadata": {},
   "source": [
    "### converts to list of sentences and words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668dd10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tag,patterns,responses,context_set\\n\\nticket,\"[\\'my problem is not solved\\', \\'you did not help me\\', \\'not a good solution\\', \\'bad solution\\', \\'not good solution\\', \\'no help\\', \\'wasted my time\\', \\'useless bot\\', \\'create a ticket\\']\",[\\'tarnsferring the request to your pm\\'],']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences\n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613d923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eadb6a",
   "metadata": {},
   "source": [
    "### The Greeting inputs and responses are given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a72f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59cfc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a040c1d",
   "metadata": {},
   "source": [
    "## Lets test a ChatBot called \"ROBO\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d46a6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit,type Bye!\n",
      "Hi\n",
      "Hi\n",
      "ROBO: hi\n",
      "Do you understand me?\n",
      "Do you understand me?\n",
      "ROBO: I am sorry! I don't understand you\n",
      "thanks\n",
      "thanks\n",
      "ROBO: You are welcome..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit,type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    print(user_response)\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e869ca",
   "metadata": {},
   "source": [
    "# NLP_PROJECT1_PART_B   -    IS OVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78423767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
